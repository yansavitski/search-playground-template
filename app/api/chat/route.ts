import { NextRequest, NextResponse } from 'next/server'
import { ChatRequest, ChatResponse } from '@/types/chat'
import { createElasticsearchClient, searchDocuments, testConnection, checkIndexExists } from '@/lib/elasticsearch'

export async function POST(request: NextRequest) {
  try {
    const body: ChatRequest = await request.json()
    const { message, config, elasticConnection, llmConfig } = body

    // Validate required fields
    if (!message || !config || !llmConfig) {
      return NextResponse.json(
        { error: 'Missing required fields' }, 
        { status: 400 }
      )
    }

    // Step 1: Search Elasticsearch for relevant documents
    const searchResults = await searchElasticsearch(message, config, elasticConnection)

    // Step 2: Generate response using LLM with context
    const llmResponse = await generateLLMResponse(message, searchResults, config, llmConfig)

    const response: ChatResponse = {
      message: llmResponse.content,
      sources: config.citations ? searchResults.hits : undefined
    }

    return NextResponse.json(response)

  } catch (error) {
    console.error('Chat API error:', error)
    return NextResponse.json(
      { error: 'Internal server error' }, 
      { status: 500 }
    )
  }
}

async function searchElasticsearch(question: string, config: any, connection: any) {
  try {
    const client = createElasticsearchClient(connection)

    const isConnected = await testConnection(client)

    if (!isConnected) {
      console.warn('Elasticsearch connection failed, using mock data')
      return { hits: [], total: 0 }
    }

    const searchResults = await searchDocuments(
      client,
      question,
      config.indices,
      config.elasticsearchQueryJSON,
      config.context?.docSize
    )

    return searchResults
  } catch (error) {
    console.error('Elasticsearch search failed:', error)
    return { hits: [], total: 0 }
  }
}

async function generateLLMResponse(query: string, searchResults: any, config: any, llmConfig: any) {
  // Build context from search results using configured source fields
  const sourceFields = config.context?.sourceFields?.default || ['title', 'body', 'content']
  
  const context = searchResults.hits
    .map((hit: any, index: number) => {
      const doc = hit._source
      let docText = `Document ${index + 1} (Score: ${hit._score?.toFixed(2)}):\n`
      
      sourceFields.forEach((field: string) => {
        if (doc[field]) {
          const fieldName = field.charAt(0).toUpperCase() + field.slice(1)
          docText += `${fieldName}: ${doc[field]}\n`
        }
      })
      
      // Add URL if available
      if (doc.url) {
        docText += `URL: ${doc.url}\n`
      }
      
      return docText
    })
    .join('\n---\n\n')

  const prompt = `${config.prompt}

Context from Elasticsearch search (found ${searchResults.hits.length} relevant documents):
${context}

User question: ${query}

Please provide a helpful answer based on the context above.${config.citations ? ' Include references to the source documents where appropriate.' : ''}`

  switch (llmConfig.provider) {
    case 'openai':
      return await callOpenAI(prompt, llmConfig)
    case 'elasticsearch':
      return await callElasticsearchML(prompt, llmConfig, config)
    case 'azure-openai':
      return await callAzureOpenAI(prompt, llmConfig)
    default:
      // Fallback to mock response
      return {
        content: `Mock response for "${query}" using ${llmConfig.provider}.\n\nBased on the search results, I found relevant information about your query. This is a simulated response that would normally be generated by ${llmConfig.provider}.`
      }
  }
}

async function callOpenAI(prompt: string, llmConfig: any) {
  if (!llmConfig.apiKey) {
    throw new Error('OpenAI API key is required')
  }

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${llmConfig.apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: llmConfig.modelName || 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
      max_tokens: 1000
    })
  })

  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.statusText}`)
  }

  const data = await response.json()
  return { content: data.choices[0].message.content }
}

async function callAzureOpenAI(prompt: string, llmConfig: any) {
  if (!llmConfig.apiKey || !llmConfig.baseUrl) {
    throw new Error('Azure OpenAI API key and base URL are required')
  }

  const response = await fetch(`${llmConfig.baseUrl}/openai/deployments/${llmConfig.modelName || 'gpt-35-turbo'}/chat/completions?api-version=2024-02-15-preview`, {
    method: 'POST',
    headers: {
      'api-key': llmConfig.apiKey,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
      max_tokens: 1000
    })
  })

  if (!response.ok) {
    throw new Error(`Azure OpenAI API error: ${response.statusText}`)
  }

  const data = await response.json()
  return { content: data.choices[0].message.content }
}

async function callElasticsearchML(prompt: string, llmConfig: any, config: any) {
  // This would use Elasticsearch's ML inference API
  // For now, return a mock response
  return {
    content: `Response using Elasticsearch ML connector ${config.summarizationModel.connectorId} with model ${config.summarizationModel.modelId}.\n\nThis would be generated using Elasticsearch's built-in ML capabilities.`
  }
}
